{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Data Prep.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.2 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "746d5ba9a74c31320cb3b9ac5aca21eaba3aaf43f4bb21f2db77c83cc1c9beae"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "from Common import Common\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from patsy import dmatrices\r\n",
        "import statsmodels.api as sm\r\n",
        "\r\n",
        "df_original = pd.read_csv('spreadsheets/BUSL_Notes.csv', skiprows=1)\r\n",
        "df_original = df_original.dropna(axis=\"rows\", how=\"all\").dropna(axis=\"columns\", how=\"all\")\r\n",
        "df_radiomics = pd.read_csv('spreadsheets/Radiomics.csv')"
      ],
      "outputs": [],
      "metadata": {
        "id": "x9tA_R6G0Nk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5cdc4d-b206-4f35-e78a-92be68675448"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop some unhelpful columns and reduce the dataset to only the numeric values"
      ],
      "metadata": {
        "id": "P46QfsOirLzG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df = df_radiomics.drop(columns = ['original_firstorder_Minimum','original_firstorder_Maximum','original_firstorder_Range'])\r\n",
        "df_numeric = df._get_numeric_data() #drop non-numeric cols\r\n",
        "df_numeric = df_numeric.drop(columns=['filename'])\r\n",
        "df_numeric.head()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "AawQA6ISq_44",
        "outputId": "0c0036b7-f52c-4a7b-dcee-5a289433dc69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare to drop any columns that have a correlation above a certain threshold (0.8 in this case)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def identify_correlated(df, threshold):\r\n",
        "    \"\"\"\r\n",
        "    A function to identify highly correlated features.\r\n",
        "    \"\"\"\r\n",
        "    # Compute correlation matrix with absolute values\r\n",
        "    matrix = df.corr().abs()\r\n",
        "    \r\n",
        "    # Create a boolean mask\r\n",
        "    mask = np.triu(np.ones_like(matrix, dtype=bool))\r\n",
        "    \r\n",
        "    # Subset the matrix\r\n",
        "    reduced_matrix = matrix.mask(mask)\r\n",
        "    \r\n",
        "    # Find cols that meet the threshold\r\n",
        "    to_drop = [c for c in reduced_matrix.columns if \\\r\n",
        "              any(reduced_matrix[c] > threshold)]\r\n",
        "    \r\n",
        "    return to_drop"
      ],
      "outputs": [],
      "metadata": {
        "id": "dcPkfh7d137y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "to_drop = identify_correlated(df_numeric, threshold=.8)\r\n",
        "df_reduced = df_numeric.drop(to_drop, axis=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "PCOBHOog1_14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "53b25945-733d-464e-8679-d281ac0a3c0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join the two data frames"
      ],
      "metadata": {
        "id": "aH-bbW6oVYJs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def expand_filename (row):\r\n",
        "  fn = row[\"filename\"]\r\n",
        "  return \"PA\" + str(fn)"
      ],
      "outputs": [],
      "metadata": {
        "id": "kWE2OYKq7SnP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_reduced[\"SampleName\"] = df.apply(lambda row: expand_filename(row), axis=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "futN1lay7RGY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# df.join(other.set_index('key'), on='key')\r\n",
        "df_joined = df_reduced.join(df_original.set_index('Sample name'), lsuffix='_rad', rsuffix='_rle', on='SampleName')"
      ],
      "outputs": [],
      "metadata": {
        "id": "6y9Azk3jVV9v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_joined.head()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_joined.to_csv('spreadsheets/joined.csv', index=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "5rt8PpY-Vrag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add the actual histology and filename back in"
      ],
      "metadata": {
        "id": "YhiqpCVkrm91"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_reduced[\"histology_actual\"] = df[\"Histology_Actual\"]\r\n",
        "df_reduced[\"histology_rle\"] = df[\"Histology_RLE\"]\r\n",
        "df_reduced[\"filename\"] = df[\"filename\"]\r\n",
        "df_reduced[\"filename\"] = df[\"filename\"]\r\n",
        "df_reduced[\"filename\"] = df[\"filename\"]\r\n",
        "df_reduced.insert(0, 'histology_rle', df_reduced.pop('histology_rle'))\r\n",
        "df_reduced.insert(0, 'histology_actual', df_reduced.pop('histology_actual'))\r\n",
        "df_reduced.insert(0, 'filename', df_reduced.pop('filename'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "CG3Q1TtCd4pP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add the shape back in"
      ],
      "metadata": {
        "id": "iQKqY8AsU5Iq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_reduced[\"ShapeACR\"] = df_joined[\"ShapeACR \"]\r\n",
        "df_reduced[\"ShapeTS\"] = df_joined[\"ShapeTS\"]"
      ],
      "outputs": [],
      "metadata": {
        "id": "9sx4RcL8U05b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the functions for quantifying the marginal zone abruptness"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# White points with at least one black neighbor\r\n",
        "def has_one_or_two_black_neighbors(gt, y, x):\r\n",
        "  arr = [gt[y, x-1] == 0, gt[y, x+1] == 0, gt[y-1, x] == 0,gt[ y+1, x] == 0]\r\n",
        "  count = arr.count(True)\r\n",
        "  return count == 1 or count == 2\r\n",
        "\r\n",
        "def get_marginal_zone_acr(bus, gt, kernel_size):\r\n",
        "  i_max = 10\r\n",
        "  prev_mean = 255\r\n",
        "  threshold = 0.95\r\n",
        "  show_images = False\r\n",
        "  img = np.uint8(gt)\r\n",
        "  for i in range(i_max):\r\n",
        "    kernel = np.ones((kernel_size,kernel_size),np.uint8)\r\n",
        "    erosion = cv2.erode(img,kernel,iterations = i)\r\n",
        "    mask = bus.copy()\r\n",
        "    # mask = (marginal % 254) * bus\r\n",
        "    mask = (erosion % 254) * bus\r\n",
        "    this_mean = mask[np.nonzero(mask)].mean()\r\n",
        "    if this_mean/prev_mean > threshold or this_mean < 15:\r\n",
        "      #print(\"stopping at \" + str(i))\r\n",
        "      return i\r\n",
        "      break\r\n",
        "    prev_mean = this_mean\r\n",
        "    if show_images:\r\n",
        "      plt.figure(figsize=(8,12))\r\n",
        "      plt.title(str(this_mean))\r\n",
        "      plt.imshow(mask, cmap=plt.cm.gray, vmax=255)\r\n",
        "  return i_max\r\n",
        "\r\n",
        "\r\n",
        "def get_abrupt_boundary_metric(bus, gt):\r\n",
        "  gy_bus, gx_bus = np.gradient(bus)\r\n",
        "\r\n",
        "  out_window = 3\r\n",
        "  in_window = 7\r\n",
        "  abrupt_threshold = 20\r\n",
        "  abrupt_pixels = 0\r\n",
        "  max_gradients = []\r\n",
        "  white_points = np.column_stack(np.where(gt == 255))\r\n",
        "  border_points = [point for point in white_points if has_one_or_two_black_neighbors(gt, point[0], point[1])]\r\n",
        "  for point in border_points:\r\n",
        "    # Get the x,y coords\r\n",
        "    y, x = point[0], point[1]\r\n",
        "    # Get the proper gradient range and look for an\r\n",
        "    v = 0\r\n",
        "    h = 0\r\n",
        "    if gt[y-1, x] == 0: # Look down for the gradient\r\n",
        "      v = 1\r\n",
        "    if gt[y+1, x] == 0:  # Look up for the gradient\r\n",
        "      v = -1\r\n",
        "    if gt[y, x-1] == 0:  # Look right for the gradient\r\n",
        "      h = 1\r\n",
        "    if gt[y, x+1] == 0:  # Look left for the gradient\r\n",
        "      h = -1\r\n",
        "\r\n",
        "    v_grad = np.empty(1)\r\n",
        "    h_grad = np.empty(1)\r\n",
        "    if v == 1:\r\n",
        "      v_grad = abs(gy_bus[y-out_window:y+in_window,x])\r\n",
        "    elif v == -1:\r\n",
        "      v_grad = abs(gy_bus[y-in_window:y+out_window,x])\r\n",
        "    if h == 1:\r\n",
        "      h_grad = abs(gx_bus[y, x-out_window:x+in_window])\r\n",
        "    elif h == -1:\r\n",
        "      h_grad = abs(gx_bus[y, x-in_window:x+out_window])\r\n",
        "\r\n",
        "    if (len(v_grad) > 0 and max(v_grad) >= abrupt_threshold) or (len(h_grad) > 0 and max(h_grad) >= abrupt_threshold):\r\n",
        "      abrupt_pixels += 1\r\n",
        "\r\n",
        "    max_gradients.append(max(abs(v_grad+h_grad)))\r\n",
        "\r\n",
        "  return abrupt_pixels/len(border_points), np.average(max_gradients)"
      ],
      "outputs": [],
      "metadata": {
        "id": "58VlEdGtqtTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and some code for quantifying orientation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_orientation(gt):\r\n",
        "\r\n",
        "  # threshold\r\n",
        "  thresh = cv2.threshold(gt, 100 , 255, cv2.THRESH_BINARY)[1]\r\n",
        "\r\n",
        "  # find largest contour\r\n",
        "  contours = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n",
        "  contours = contours[0] if len(contours) == 2 else contours[1]\r\n",
        "  big_contour = max(contours, key=cv2.contourArea)\r\n",
        "\r\n",
        "  # fit contour to ellipse and get ellipse center, minor and major diameters and angle in degree \r\n",
        "  # the angle here refers to the angle with the middle of the x-axis, so it could run \r\n",
        "  # from 0 (straight to the left) to 180 degrees (straight to the right), with 90 degrees being perpendicular to the x-axis\r\n",
        "  ellipse = cv2.fitEllipse(big_contour)\r\n",
        "  (xc,yc),(d1,d2),angle = ellipse\r\n",
        "\r\n",
        "  # We are really oly interested in the angle's absolute deviation from vertical.\r\n",
        "  angle = abs(90-angle)\r\n",
        "  return angle"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add columns to the data frame for abruptness and orientation"
      ],
      "metadata": {
        "id": "_hIdbE-irsp4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "root_dir = '/content/gdrive/MyDrive/BUS_Project_Home/Share_with_group/Adam_Silberfein'\r\n",
        "import sys\r\n",
        "sys.path.append(root_dir)\r\n",
        "import os\r\n",
        "\r\n",
        "for i, row in df_reduced.iterrows():\r\n",
        "  filename = str(row[\"filename\"]).zfill(3)\r\n",
        "\r\n",
        "  try:\r\n",
        "    bus, gt, og = Common.get_images(row[\"filename\"])\r\n",
        "    marginal_zone = get_marginal_zone_acr(bus, gt, 5)\r\n",
        "    abruptness, avg_max_gradient = get_abrupt_boundary_metric(bus, gt)\r\n",
        "    orientation = get_orientation(gt)\r\n",
        "    df_reduced.at[i, \"abruptness\"] = abruptness\r\n",
        "    df_reduced.at[i, \"avg_max_gradient\"] = avg_max_gradient\r\n",
        "    df_reduced.at[i, \"orientation\"] = orientation\r\n",
        "  except:\r\n",
        "    print(\"Error on \", filename)\r\n",
        "    continue\r\n",
        "  \r\n",
        "  "
      ],
      "outputs": [],
      "metadata": {
        "id": "a9MUnT4NrrMx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_reduced = df\r\n",
        "for i, row in df_reduced.iterrows():\r\n",
        "  filename = str(row[\"filename\"]).zfill(3)\r\n",
        "  print(filename)\r\n",
        "  bus, gt, og = Common.get_images(filename)\r\n",
        "  orientation = get_orientation(gt)\r\n",
        "  df_reduced.at[i, \"orientation\"] = orientation\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "O1ikQd3HsVir"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_reduced"
      ],
      "outputs": [],
      "metadata": {
        "id": "E5QI0kPhfjiD"
      }
    }
  ]
}